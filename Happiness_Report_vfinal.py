import streamlit as st
from matplotlib import _preprocess_data
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go


from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, max_error, mean_absolute_error, r2_score
import warnings
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from PIL import Image
warnings.filterwarnings('ignore')

# Configuration de la page principale
st.set_page_config(
    page_title="Projet de Pr√©diction du Score de Bonheur",
    page_icon=None,
    layout="centered",
    initial_sidebar_state="collapsed"
)
st.markdown("<h1 style='text-align: center;'>‚ú®Projet de Pr√©diction du Score de Bonheur‚ú®</h1>", unsafe_allow_html=True)

st.sidebar.title("üß≠ Navigation")
pages = {
    "Page de bienvenue": "Explorez le projet et comprenez ses objectifs.",
    "Pr√©-traitement des donn√©es": "D√©tails sur la pr√©paration et le nettoyage des donn√©es.",
    "Visualisation": "D√©couvrez les graphiques et les insights visuels.",
    "Mod√©lisation": "Examinez les mod√®les utilis√©s et leurs performances.",
    "Machine Learning": "Explorez l'impl√©mentation des algorithmes.",
    "Conclusions et Perspectives": "R√©sum√© et discussion sur les perspectives.",
    "Auteurs": "Pr√©sentation de l'√©quipe du projet."
}

# Initialisation de la page s√©lectionn√©e dans la session Streamlit
if 'selected_page' not in st.session_state:
    st.session_state.selected_page = None

# Affichage des boutons de navigation et descriptions dans la barre lat√©rale
for page, description in pages.items():
    if st.sidebar.button(page):
        st.session_state.selected_page = page
    # Affichage de la description sous chaque bouton
    st.sidebar.write(f"‚ÑπÔ∏è {description}")

# Ajout du bouton "Retour"
if st.sidebar.button("‚¨ÖÔ∏è Retour"):
    st.session_state.selected_page = None

# Chargement des donn√©es et pr√©paration des fonctions de pages
@st.cache_data
def charger_donnees():
    df = pd.read_csv('world-happiness-report.csv')
    df2 = pd.read_csv('world-happiness-report-2021.csv')
    df_energy = pd.read_csv('owid-energy-data.csv')
    return df, df2, df_energy

df, df2, df_energy = charger_donnees()

@st.cache_data
def data_preparation(df, df2, df_energy):

    df=df.merge(df2[['Country name','Regional indicator']],on='Country name',how='left')

    dictionnaire = {'Ladder score': 'Life Ladder',
                        'Logged GDP per capita': 'Log GDP per capita',
                        'Social support'      : 'Social support',
		            	      'Healthy life expectancy'      : 'Healthy life expectancy at birth',
			                  'Freedom to make life choices'      : 'Freedom to make life choices',
			                  'Generosity'      : 'Generosity',
                        'Perceptions of corruption'       : 'Perceptions of corruption'}

    df2= df2.rename(dictionnaire, axis = 1)

    df3 = df2.drop(['Standard error of ladder score', 'upperwhisker', 'lowerwhisker','Ladder score in Dystopia','Explained by: Log GDP per capita','Explained by: Social support','Explained by: Healthy life expectancy',
                'Explained by: Freedom to make life choices','Explained by: Generosity','Explained by: Perceptions of corruption','Dystopia + residual'], axis=1)

    df3['year']=2021
    df3.head()

    df4 = pd.concat([df, df3], ignore_index=True)

    dictionnaire = {'Life Ladder': 'Life_Ladder',
                        'Log GDP per capita': 'Log_GDP_per_capita',
                        'Social support'      : 'Social_support',
		            	      'Healthy life expectancy at birth'      : 'Healthy_life_expectancy_at_birth',
			                  'Freedom to make life choices'      : 'Freedom_to_make_life_choices',
			                  'Generosity'      : 'Generosity',
                        'Perceptions of corruption  '       : 'Perceptions_of_corruption'}

    df4 = df4.rename(dictionnaire, axis = 1)
   
    df_energy.rename(columns ={'country' : 'Country name'}, inplace = True)

    df_energy = df_energy[['year','Country name','population', 'energy_per_capita']]

    df_final = df4.merge(df_energy, on=['Country name', 'year'], how='inner')
    df_final = df_final[df_final['year'] != 2005]

    region_dict = {
    'Angola': 'Sub-Saharan Africa',
    'Central African Republic': 'Sub-Saharan Africa',
    'Djibouti': 'Sub-Saharan Africa',
    'Somalia': 'Sub-Saharan Africa',
    'South Sudan': 'Sub-Saharan Africa',
    'Sudan': 'Sub-Saharan Africa',
    'Belize': 'Latin America and Caribbean',
    'Cuba': 'Latin America and Caribbean',
    'Guyana': 'Latin America and Caribbean',
    'Suriname': 'Latin America and Caribbean',
    'Trinidad and Tobago': 'Latin America and Caribbean',
    'Bhutan': 'South Asia',
    'Oman': 'Middle East and North Africa',
    'Qatar': 'Middle East and North Africa',
    'Syria': 'Middle East and North Africa'}

    df_final['Regional indicator'] = df_final['Country name'].map(region_dict).fillna(df_final['Regional indicator'])

    df_final['Positive affect'] = df_final['Positive affect'].fillna(0)
    df_final['Negative affect'] = df_final['Negative affect'].fillna(0)

    return df_final

df_final = data_preparation(df, df2, df_energy)

@st.cache_data
def page_pretraitement_donnees(df_final):

    feature = df_final.drop(columns=['Life_Ladder'])
    target = df_final['Life_Ladder']

    X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=0.2, random_state=42)

    numeric_features = ['Log_GDP_per_capita', 'Social_support', 'Healthy_life_expectancy_at_birth',
                'Freedom_to_make_life_choices', 'Generosity', 'Perceptions of corruption',
                'Positive affect', 'Negative affect', 'population', 'energy_per_capita']

    imputer = SimpleImputer(missing_values = np.nan, strategy = 'median') 

    X_train[numeric_features] = imputer.fit_transform(X_train[numeric_features])
    X_test[numeric_features] = imputer.transform(X_test[numeric_features])

    cat_features = ['Regional indicator']
    cat_train = X_train[cat_features]
    cat_test = X_test[cat_features]

    oneh = OneHotEncoder (drop = 'first', sparse_output = False, handle_unknown = 'ignore')

    encoded_train = oneh.fit_transform(X_train[cat_features])
    encoded_test = oneh.transform(X_test[cat_features])

    encoded_cat_train = pd.DataFrame(encoded_train, columns = oneh.get_feature_names_out(cat_train.columns))
    encoded_cat_test = pd.DataFrame(encoded_test, columns = oneh.get_feature_names_out(cat_test.columns))

    numeric_train = X_train[numeric_features]
    numeric_test = X_test[numeric_features]

    scaler = StandardScaler()

    numeric_train_scaled = scaler.fit_transform(numeric_train)
    numeric_test_scaled = scaler.transform(numeric_test)

    numeric_train_scaled = pd.DataFrame(numeric_train_scaled, columns=numeric_features)
    numeric_test_scaled = pd.DataFrame(numeric_test_scaled, columns=numeric_features)
    
    X_train_final = pd.concat([numeric_train_scaled, encoded_cat_train], axis=1)
    X_test_final = pd.concat([numeric_test_scaled, encoded_cat_test], axis=1)

    return X_train_final, X_test_final, y_train, y_test

X_train_final, X_test_final, y_train, y_test = page_pretraitement_donnees(df_final)

# Affichage de la page s√©lectionn√©e
if st.session_state.selected_page:
    st.header(f" {st.session_state.selected_page}")

    # Contenu de chaque page
    if st.session_state.selected_page == "Page de bienvenue":
        st.write("### Bienvenue sur le projet de pr√©diction du score de bonheur")
        st.write("""
        Au cours de ce projet nous allons effectuer une analyse approfondie des donn√©es collect√©es 
        par le **World Happiness Report** afin de r√©pondre √† une question qui pourrait para√Ætre existentielle :
        ‚Äúqu‚Äôest-ce que le bonheur ?‚Äù ou alors ‚Äúqu‚Äôest ce qui me rend heureux ? 
        Pas facile de r√©pondre √† cette question, vous me direz, nous allons donc tourner cette question d‚Äôune 
        autre mani√®re qui est la suivante : ‚ÄúLe bonheur est-il mesurable ?‚Äù

        ### Contexte du projet
        Ce projet a pour objectif de pr√©dire le **score de bonheur** des pays en s'appuyant sur des donn√©es socio-√©conomiques et √©nerg√©tiques obtenus depuis Kaggle.
        Notre analyse s'est concentr√©e sur l'influence de diff√©rents indicateurs, tels que :
        - Le Produit Int√©rieur Brut (PIB),
        - L'esp√©rance de vie,
        - Le support social,
        - La consommation d'√©nergie par habitant,
        - Et d'autres indicateurs r√©gionaux.
         
        ### Objectifs du projet
        - **Analyser** les corr√©lations entre diff√©rents indicateurs et le bien-√™tre des populations.
        - **D√©velopper des mod√®les pr√©dictifs** performants en testant plusieurs algorithmes de machine learning.
        - **Optimiser les mod√®les choisis** pour garantir des pr√©dictions robustes et adapt√©es √† des √©tudes socio-√©conomiques futures.

        ### M√©thodologie
        Nous avons test√© cinq mod√®les initiaux :
        - R√©gression Lin√©aire
        - Support Vector Regressor (SVR)
        - Random Forest Regressor
        - Gradient Boosting Regressor (GBR)
        - XGBoost Regressor
        """)
        st.write("Nous esp√©rons que ce projet vous inspirera pour explorer de nouvelles pistes d'analyse socio-√©conomiques. Bonne d√©couverte!")

    elif st.session_state.selected_page == "Pr√©-traitement des donn√©es":
        st.write("""
        #### Ci dessous les √©tapes de preprocessing  appliqu√©es √† notre jeu de donn√©es
        
        ###### Le dataset original de Kaggle
        Nous avons d√©cid√© de retirer l'ann√©e 2005, car les donn√©es sont limit√©es cette ann√©e l√† ( nombre de pays sond√©s tr√®s faible compar√© aux autres ann√©es)""") 
        
        if st.checkbox("Afficher √©chantillon du dataset original pour l'ann√©e 2021"):
            st.write(df2.head(10))


        st.write("""
        ###### Ajout de donn√©es externes
        Enrichissement avec deux variables explicatives externes recherch√©es sur des dataset externes, qui nous ont sembl√©es int√©ressantes quant √† leur impact sur le score du bonheur : population du pays et  niveau d‚Äô√©nergie consomm√© par habitant par pays

                 
        **Notre jeu de donn√©es final**, apr√®s combinaison des donn√©es pr√©cit√©es, se compose de 
        - la variable cible : Life Ladder, repr√©sentant le score de bonheur √† pr√©dire.
        - 13 variables explicatives, s√©lectionn√©es pour leur pertinence""") 
        # Cr√©er un buffer pour capturer la sortie de df.info()
        import io
        buffer = io.StringIO()
        df_final.info(buf=buffer)
        info = buffer.getvalue()
        st.text(info)

        st.write("""
        ###### Transformation
        Nous avons identifi√© les transformations suivantes, qui ont √©t√© appliqu√©es apr√®s le train test split afin d'√©viter la fuite de donn√©es
        - V√©rification des doublons
        - Remplacement des valeurs manquantes par la m√©diane , pour les variables num√©riques
        - Encodage de la variable  cat√©gorielle "Regional Indicator"
        - Normalisation des variables continues""") 

        st.write("Les jeux d'entrainement et de test sont pr√™ts pour l'√©tape de mod√©lisation")

        st.write(X_train_final.head(10))

        
    elif st.session_state.selected_page == "Visualisation":
        st.write("### D√©couvrez les visualisations de donn√©es")
        st.write("Explorez les diff√©rentes repr√©sentations visuelles des donn√©es pour mieux comprendre leurs relations et distributions.")
        
        # Menu d√©roulant pour choisir la visualisation
        option = st.selectbox(
        "S√©lectionnez une visualisation",
        ("Carte Choropl√®the", "Matrice de corr√©lation", "Distribution de la variable cible", "Boxplot de la distribution des variables", "Top & Worst 5"))
    
        # Carte Choropl√®the
        if option == "Carte Choropl√®the":
            st.subheader("Carte Choropl√®the")
            st.write("Pour illustrer l'√©volution du score de bonheur √† travers les pays et sur plusieurs ann√©es, nous avons utilis√© une carte choropl√®the. Cette visualisation permet de repr√©senter les disparit√©s g√©ographiques de bien-√™tre √† l‚Äô√©chelle mondiale : ")
            fig = px.choropleth(df_final.sort_values("year"), locations="Country name", locationmode="country names", color="Life_Ladder",
                             animation_frame = "year", title="Score de bonheur par pays")
            st.plotly_chart(fig)
            st.write("""
                     On y observe plusieurs tendances int√©ressantes :

                     - **√âtats-Unis** : diminution progressive du score de bonheur au fil des ann√©es.
                     - **Australie** :  score relativement stable et globalement positif.
                     - **Russie**, Asie centrale et Asie :  scores moyens qui restent √©galement stables au cours du temps.
                     - **Afrique** :  am√©lioration lente mais continue du score de bonheur au fil des ann√©es.
                     - **Europe** : Scores globalement √©lev√©s et stables, avec des r√©sultats particuli√®rement remarquables pour les pays scandinaves.
                     
                     L'ann√©e 2013 se distingue de fa√ßon positive pour l'ensemble des pays.
                     Enfin, on note une l√©g√®re baisse des scores de bonheur en 2020, probablement en raison de la pand√©mie de Covid-19.
                     """)

        # Matrice de corr√©lation
        elif option == "Matrice de corr√©lation":
            st.subheader("Matrice de corr√©lation des variables explicatives")
            st.write("""Un probl√©matique crucial est de v√©rifier si certaines variables explicatives pr√©sentent une forte corr√©lation entre elles, ce qui pourrait entra√Æner des probl√®mes de **multicolin√©arit√©** lors de la mod√©lisation. 
Une multicolin√©arit√© √©lev√©e pourrait non seulement nuire √† la pr√©cision du mod√®le, mais aussi compliquer l'interpr√©tation des r√©sultats. Pour explorer cette hypoth√®se, nous allons visualiser **une matrice de corr√©lation (heatmap)** :
""")
            fig, ax = plt.subplots(figsize=(10, 8))
            sns.heatmap(X_train_final.corr(), annot=True, cmap='coolwarm', fmt=".2f", ax=ax)
            st.pyplot(fig)
            st.write("""Nous avons constat√© **une forte corr√©lation entre plusieurs variables explicatives**, notamment le PIB par habitant, l'esp√©rance de vie et le soutien social. 
                     Cette interd√©pendance peut entra√Æner des probl√®mes de **multicolin√©arit√©** lors de la mod√©lisation.""")

        # Distribution de la variable cible
        elif option == "Distribution de la variable cible":
            st.subheader("Distribution de la variable cible - Score de bonheur (Life Ladder)")
            st.write("""La distribution de la variable cible **Life Ladder** nous permets de d√©terminer 
                     si les extr√™mes diminuent et si les scores se resserrent davantage autour de la m√©diane, 
                     ce qui pourrait indiquer une convergence progressive des niveaux de bonheur entre les pays. 
                     Globalement, **le score de bonheur se situe entre 4,6 et 7,2**. 
                     """)
            fig, ax = plt.subplots()
            sns.histplot(df_final['Life_Ladder'], kde=True, bins=20, color='skyblue', ax=ax)
            st.pyplot(fig)

        # Boxplot de la distribution des variables explicatives
        elif option == "Boxplot de la distribution des variables":
            st.subheader("Boxplot de la distribution des variables explicatives")
            st.write("""Nous observons que certains variables explicatives suivent des tendances similaires,
                      ce qui pourrait indiquer **une relation √©troite entre elles**. 
                     Cela sugg√®re que les variables comme le PIB pourrait jouer un r√¥le pr√©pond√©rant dans la pr√©diction du bonheur. """)
            numeric_features = ['Log_GDP_per_capita', 'Social_support', 'Healthy_life_expectancy_at_birth',
                'Freedom_to_make_life_choices', 'Generosity', 'Perceptions of corruption',
                'Positive affect', 'Negative affect', 'population', 'energy_per_capita']
            features = X_train_final[numeric_features]
            fig, ax = plt.subplots(figsize=(12, 6))
            sns.boxplot(data=features, ax=ax)
            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")
            st.pyplot(fig)
                     
        # Composition des r√©gions
        elif option == "Top & Worst 5":
            st.subheader("Pays avec le meilleur et le pire score de bonheur")
            st.write("Nous avons √©galement identifi√© les pays ayant les **meilleurs** et les **pires scores de bonheur** en moyenne au fil des ann√©es : ")
            df_grouped = df_final.groupby('Country name')['Life_Ladder'].mean().reset_index()
            df_sorted = df_grouped.sort_values('Life_Ladder', ascending = False)
            top_5 = df_sorted.head(5)
            worst_5 = df_sorted.tail(5)
            top_worst = pd.concat([top_5, worst_5])
            colors = ['green'] * len(top_5) + ['red'] * len(worst_5)
            
            fig = go.Figure(go.Bar(x=top_worst['Life_Ladder'], y=top_worst['Country name'], orientation='h', marker_color=colors))
            fig.update_layout(title="Pays avec le meilleur et le pire score de bonheur", xaxis_title="Score de bonheur", yaxis_title="Pays", showlegend=False)

            st.plotly_chart(fig)
            st.write("Ce graphique nous montre **un √©cart tr√®s important entre les pays**, qui doit donc pouvoir s‚Äôexpliquer par l‚Äôensemble des variables mis √† notre disposition dans le data frame")

    elif st.session_state.selected_page == "Mod√©lisation":
        st.subheader("Pr√©sentation des mod√®les et leur performance")
        st.write("""Nous avons choisi cinq mod√®les pour pr√©dire le score du bonheur, dont la nature du probl√®me est de type r√©gression. 
                \n Les mod√®les sont ensuite √©valu√©s √† l'aide des m√©triques standards :
                \n -**MAE** (Mean Absolute Error) : Mesure l‚Äôerreur moyenne entre les valeurs r√©elles et les pr√©dictions sans tenir compte du signe.
                \n -**MSE** (Mean Squared Error) : Donne une importance plus grande aux erreurs importantes en √©levant chaque diff√©rence au carr√©.
                \n -**RMSE** (Root Mean Squared Error) : Indique l'√©cart type des r√©sidus, offrant une interpr√©tation plus facile de la pr√©cision du mod√®le en utilisant les m√™mes unit√©s que la variable cible.
                \n -**R¬≤** : Repr√©sente la proportion de variance de la variable cible expliqu√©e par les variables explicatives.
                """)
        st.text("")
        st.text("")
        st.text("")
        # Menu d√©roulant pour choisir la visualisation
        option = st.selectbox(
        "**S√©lectionnez un modele**",
        ("R√©gression lin√©aire", "Support Vector Regression SVR", "Random Forest Regressor", "Gradient Boosting Regressor", "XGboost"))
        st.text("")
        st.text("")

        # R√©gression lin√©aire
        if option == "R√©gression lin√©aire":
            st.write("**R√©gression lin√©aire**")
            st.write("Un mod√®le simple et interpr√©table qui suppose une relation lin√©aire entre les variables explicatives et la variable cible du bien √™tre √† l‚Äô√©chelle mondiale : ")
            st.write(""" **R√©sultats**:
                    \n - Score train : 0.7995
                    \n - Score test : 0.7812
                    \n - MSE : 0.2648
                    \n - MAE : 0.3952
                    \n - RMSE : 0.5146
                     """)
            st.text("")
            st.write("""**Graphique sur les valeurs r√©elles versus valeurs pr√©dites (gauche)**
                     \n **Graphique sur les Feature Importance de r√©gression lin√©aire (droite)**""")
            # Afficher l'image dans Streamlit
            FI_regression_lineaire = Image.open("FI_regression_lineaire.png")
            st.image(FI_regression_lineaire, use_column_width=True)
            st.text("")
            st.text("")
            st.text("")
            st.text("")

        # Support Vector Regression SVR
        elif option == "Support Vector Regression SVR":
            st.write("**Support Vector Regression SVR**")
            st.write("Un mod√®le plus flexible, bas√© sur les marges, qui est particuli√®rement efficace dans les situations o√π la relation entre les variables n'est pas strictement lin√©aire.")
            st.write(""" R√©sultats:
                    \n - Score train : 0.8924
                    \n - Score test : 0.8634
                    \n - MSE : 0.1653
                    \n - MAE : 0.3107
                    \n - RMSE : 0.4066
                     """)
            st.text("")
            st.write("""**Graphique sur les valeurs r√©elles versus valeurs pr√©dites**
                     \n ****""")
            # Afficher l'image dans Streamlit
            FI_SVR = Image.open("FI_SVR.png")
            st.image(FI_SVR, use_column_width=True)
            st.text("")
            st.text("")
            st.text("")
            st.text("")

        # Random Forest Regressor
        elif option == "Random Forest Regressor":
            st.write("**Random Forest Regressor**")
            st.write("Un mod√®le bas√© sur des ensembles d'arbres de d√©cision, robuste aux donn√©es bruit√©es et efficace pour capturer les interactions non lin√©aires entre les variables")
            st.write(""" R√©sultats:
                    \n - Score train : 0.9858
                    \n - Score test : 0.8923
                    \n - MSE : 0.1304
                    \n - MAE : 0.2718
                    \n - RMSE : 0.3611
                     """)
            st.text("")
            st.write("""**Graphique sur les valeurs r√©elles versus valeurs pr√©dites (gauche)**
                     \n **Graphique sur les Feature Importance de Random Forest (droite)**""")
            # Afficher l'image dans Streamlit
            FI_RForest = Image.open("FI_RForest.png")
            st.image(FI_RForest, use_column_width=True)
            st.text("")
            st.text("")
            st.text("")
            st.text("")

        # Gradient Boosting Regressor
        elif option == "Gradient Boosting Regressor":
            st.write("**Gradient Boosting Regressor**")
            st.write("Un mod√®le bas√© sur des arbres de d√©cision successifs, o√π chaque nouvel arbre corrige les erreurs des pr√©c√©dents, ce qui le rend efficace pour r√©duire les erreurs r√©siduelles de mani√®re it√©rative")
            st.write(""" R√©sultats:
                    \n - Score train : 0.9085
                    \n - Score test : 0.8580
                    \n - MSE : 0.1718
                    \n - MAE : 0.3212
                    \n - RMSE : 0.4145
                     """)
            st.text("")
            st.write("""**Graphique sur les valeurs r√©elles versus valeurs pr√©dites (gauche)**
                     \n **Graphique sur les Feature Importance de Gradient Boosting (droite)**""")
            # Afficher l'image dans Streamlit
            FI_GBR = Image.open("FI_GBR.png")
            st.image(FI_GBR, use_column_width=True)
            st.text("")
            st.text("")
            st.text("")
            st.text("")
                     
        # XGboost
        elif option == "XGboost":
            st.write("**XGboost**")
            st.write("Une version optimis√©e du Gradient Boosting, plus rapide et souvent plus performante gr√¢ce √† son utilisation d'optimisations comme la r√©gularisation")
            st.write(""" R√©sultats:
                    \n - Score train : 0.9990
                    \n - Score test : 0.8851
                    \n - MSE : 0.1391
                    \n - MAE : 0.2798
                    \n - RMSE : 0.3729
                     """)
            st.text("")
            st.write("""**Graphique sur les valeurs r√©elles versus valeurs pr√©dites (gauche)**
                     \n **Graphique sur les Feature Importance de XGBoost (droite)**""")
            # Afficher l'image dans Streamlit
            FI_XGB = Image.open("FI_XGB.png")
            st.image(FI_XGB, use_column_width=True)
            st.text("")
            st.text("")
            st.text("")
            st.text("")

        st.write("""
        ## R√©capitulatif des performances des mod√®les   

        Ci-dessous, le tableau r√©capitulatif des performances de nos cinq mod√®les de r√©gression selon les m√©triques √©voqu√©s: le score d'entra√Ænement et de test (R¬≤), ainsi que les erreurs MSE, MAE et RMSE :""") 
        # Cr√©ation du DataFrame des metrics 
        data = {
        '': ['Score train', 'Score test  R¬≤', 'MSE','MAE','RMSE'],
        'Linear Regression': [0.7995,0.7812,0.2648,0.3952,0.5146],
        'SVR': [0.8924,0.8634,0.1653,0.3107,0.4066],
        'Random Forest': [0.9858,0.8923,0.1304,0.2718,0.3611],
        'GBR': [0.9085, 0.8580, 0.1718,0.3212,0.4145],
        'XGB': [0.9990,0.8851,0.1391,0.2798,0.3729],
        }
        metrics = pd.DataFrame(data)
        st.dataframe(metrics)

        st.write("""Parmi nos mod√®les, le Random Forest a fourni le meilleur r√©sultat, toutefois nous avons observ√© que ce mod√®le d√©pendait fortement d'une seule variable (PIB).Cette concentration excessive sur une variable peut 
                biaiser les r√©sultats et limiter la capacit√© du mod√®le √† prendre en compte d‚Äôautres facteurs. 
                \n En revanche, le Gradient Boosting Regressor s‚Äôest montr√© plus √©quilibr√© dans la feature importance, ce qui nous incite √† privil√©gier ce mod√®le.
                \n
                \n Pour aller plus loin dans l'am√©lioration de ces algorithmes, c'est √† dire en optimiser les hyperparam√®tres, nous avons retenu les mod√®les  **Gradient Boosting Regressor** et **XGBoost Regressor**, 
                plus complexes mais offrant un panel plus important d'optimisation de param√®tres.
                \n
                \n Afin d'am√©liorer les deux mod√®les GBR et XGBoost, nous avons utilis√© une recherche avec Gridsearch  en appliquant des valeurs sur les param√®tres comme le learning_rate, max_depth, min_samples_leaf,
                  min_samples_split, n_estimators, subsample, colsample_bytree. Ces param√®tres optimis√©s ont permis de renforcer la performance des mod√®les.""")

        X_train_copy = X_train_final.copy()
        y_train_copy = y_train.copy()
        X_test_copy = X_test_final.copy()
        y_test_copy = y_test.copy()
        

        # Ajuster le StandardScaler sur les donn√©es d'entra√Ænement et l'enregistrer dans st.session_state (pour standardiser les valeurs donn√©es par visiteurs de l'app)
        scaler = StandardScaler().fit(X_train_final)
        st.session_state['scaler'] = scaler

        # Initialisation des mod√®les 
        gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
        xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

        # Gradient Boosting Regressor
        gbr.fit(X_train_copy, y_train_copy)
        y_pred_gbr = gbr.predict(X_test_copy)
        mse_gbr = mean_squared_error(y_test_copy, y_pred_gbr)
        r2_gbr = r2_score(y_test_copy, y_pred_gbr)


        # XGBoost Regressor
        xgb.fit(X_train_copy, y_train_copy)
        y_pred_xgb = xgb.predict(X_test_copy)
        mse_xgb = mean_squared_error(y_test_copy, y_pred_xgb)
        r2_xgb = r2_score(y_test_copy, y_pred_xgb)


        # les mod√®les pour les utiliser dans la pr√©diction
        st.session_state['gbr_model'] = gbr
        st.session_state['xgb_model'] = xgb

        #afficher les scores des mod√®les optimis√©s
        if st.checkbox("Afficher les scores des deux mod√®les optimis√©s"):
            # Cr√©ation du DataFrame des r√©sultats
            data2 = {
            '': ['Score train', 'Score test  R¬≤', 'MSE','MAE','RMSE'],
            'GBR optimis√©': [0.9851,0.8876,0.1360,0.2839,0.3688],
            'XGB optimis√©': [0.9996, 0.8986, 0.1227,0.2602,0.3504]
                }
            metrics2 = pd.DataFrame(data2)
            st.dataframe(metrics2)


        
    elif st.session_state.selected_page == "Machine Learning":
        st.write("Sur cette page, vous pouvez explorer notre **algorithme de machine learning** en testant la pr√©cision de pr√©diction du score de bonheur des pays.") 
        st.write("Vous avez la possibilit√© d‚Äôutiliser des donn√©es issues du **World Happiness Report** pour v√©rifier la pr√©cision des pr√©dictions, ou bien de tester avec des valeurs plus r√©centes, en vous basant sur des informations disponibles en ligne pour estimer un score de bonheur √† venir.") 
        st.write("Pas de souci pour le format des donn√©es : notre algorithme standardisera automatiquement les valeurs saisies avant de lancer la pr√©diction.")
    
        if 'scaler' not in st.session_state:
            scaler = StandardScaler().fit(X_train_final)
            st.session_state['scaler'] = scaler 
        else:
            scaler = st.session_state['scaler'] 

        # Configuration des mod√®les
        def simulate_prediction(model, features):
            """Simule la pr√©diction pour les valeurs souhait√©es en standardisant les entr√©es."""
            scaler = st.session_state['scaler']
            standardized_features = scaler.transform([features])
            prediction = model.predict(standardized_features)
            return prediction[0]

        # Variables explicatives et valeurs de l'exemple 2022 (France)
        feature_names = ['PIB par habitant', 'Soutien Social', 'Esp√©rance de vie', 'Libert√© de faire des choix de vie',
                     "G√©n√©rosit√©", "Perceptions de la corruption", "Affect positif de l‚Äôann√©e (0 si vous connaissez pas)",
                     "Affect n√©gatif de l‚Äôann√©e (0 si vous connaissez pas)", "Population", 
                     "Consommation d‚Äô√©nergie par habitant", "Appartenance √† la r√©gion (1 ou 0) : Communaut√© des √âtats ind√©pendants",
                     "Appartenance √† la r√©gion (1 ou 0) : Asie de l'Est", "Appartenance √† la r√©gion (1 ou 0) : Am√©rique latine et Cara√Øbes", 
                     "Appartenance √† la r√©gion (1 ou 0) : Moyen-Orient et Afrique", 
                     "Appartenance √† la r√©gion (1 ou 0) : Am√©rique du Nord et ANZ (Australie et Nouvelle-Z√©lande)", 
                     "Appartenance √† la r√©gion (1 ou 0) : Asie du Sud", "Appartenance √† la r√©gion (1 ou 0) : Asie du Sud-Est",
                     "Appartenance √† la r√©gion (1 ou 0) : Afrique subsaharienne", "Appartenance √† la r√©gion (1 ou 0) : Europe de l‚ÄôOuest"]

        example_values = [1.863, 1.219, 0.808, 0.567, 0.07, 0.266, 0, 0, 64626624, 36051, 0, 0, 0, 0, 0, 0, 0, 0, 1]

        # Affichage de l'exemple 2022 et option de remplissage automatique
        st.info("Pour vous donner une id√©e, les valeurs pour la France en 2022 sont affich√©es ci-dessous. En cochant la case, vous acc√©derez √† l'extrait des donn√©es issues des rapports Kaggle que nous avons utilis√©. Appuyez sur le bouton ci-dessous pour remplir automatiquement les champs avec les donn√©es de cet exemple, ou bien saisissez vos propres valeurs pour simuler la pr√©diction du score de bonheur.")
        if st.checkbox("Afficher l'exemple 2022 (France)"):
            st.image("France 2022.png")

        if st.button("Remplir les champs avec les valeurs de l'exemple (2022 - France)"):
            for i, feature in enumerate(feature_names):
                st.session_state[f"gbr_{feature}"] = example_values[i]
                st.session_state[f"xgb_{feature}"] = example_values[i]

        # Cr√©ation des onglets pour chaque mod√®le
        tabs = st.tabs(["Gradient Boosting Regressor", "XGBoost Regressor"])

        # Premier onglet - Gradient Boosting Regressor
        with tabs[0]:
            st.subheader("Simulation avec Gradient Boosting Regressor")
            user_inputs_gbr = []
            for feature in feature_names:
                value = st.session_state.get(f"gbr_{feature}", 0.0)
                input_value = st.number_input(f"Saisir la valeur de {feature} :", value=value, key=f"gbr_{feature}")
                user_inputs_gbr.append(input_value)

            if 'gbr_model' in st.session_state:
                if st.button("Simuler la pr√©diction (GBR)"):
                    gbr_prediction = simulate_prediction(st.session_state['gbr_model'], user_inputs_gbr)
                    st.write(f"**Score de bonheur pr√©dit avec GBR :** {gbr_prediction:.2f}")
            else:
                st.write("GBR Non disponible")

        # Deuxi√®me onglet - XGBoost Regressor
        with tabs[1]:
            st.subheader("Simulation avec XGBoost Regressor")
            user_inputs_xgb = []
            for feature in feature_names:
                
                value = st.session_state.get(f"xgb_{feature}", 0.0)
                input_value = st.number_input(f"Saisir la valeur de {feature} :", value=value, key=f"xgb_{feature}")
                user_inputs_xgb.append(input_value)

            if 'xgb_model' in st.session_state:
                if st.button("Simuler la pr√©diction (XGB)"):
                    xgb_prediction = simulate_prediction(st.session_state['xgb_model'], user_inputs_xgb)
                    st.write(f"**Score de bonheur pr√©dit avec XGB :** {xgb_prediction:.2f}")
            else:
                st.write("XGB Non disponible")



    elif st.session_state.selected_page == "Conclusions et Perspectives":
        st.write("### R√©sum√© des r√©sultats obtenus et perspectives futures.")
        st.write("""
                 Apr√®s ajustement des hyperparam√®tres, les deux mod√®les optimis√©s ‚Äî **Gradient Boosting Regressor (GBR)** et **XGBoost Regressor (XGB)** ‚Äî ont d√©montr√© des performances √©lev√©es, notamment en termes de g√©n√©ralisation. 
                 
                 **Le mod√®le XGBoost**, en particulier, a obtenu des r√©sultats l√©g√®rement sup√©rieurs avec **des erreurs moyennes r√©duites** et **une meilleure explication de la variabilit√© de la variable cible**. 
                 De plus, XGBoost a r√©v√©l√© **une plus grande sensibilit√© aux indicateurs g√©ographiques**, notamment en identifiant certaines r√©gions, comme l‚ÄôAm√©rique Latine et l‚ÄôAfrique Subsaharienne, comme des facteurs influents dans le score de bonheur. 
                 
                 Cela constitue un r√©sultat surprenant, sugg√©rant que l‚Äôinfluence g√©ographique, au-del√† des indicateurs √©conomiques classiques comme le PIB, peut jouer un r√¥le significatif dans les perceptions de bien-√™tre.
                 
                 Ce constat soul√®ve plusieurs hypoth√®ses pour de futures recherches. 
                 L‚Äôimpact √©lev√© de certaines r√©gions pourrait, par exemple, √™tre li√© √† des facteurs culturels, sociaux ou m√™me environnementaux propres √† ces zones, qui ne sont pas explicitement int√©gr√©s dans notre jeu de donn√©es.""")

        st.write( "### Difficult√©s Scientifiques et Techniques Rencontr√©s")
        st.write("""
                La s√©lection et la gestion des variables explicatives ont constitu√© le principal d√©fi de ce projet. 
                 
                Bien que l‚Äôensemble de donn√©es comporte diverses variables √©conomiques et sociales, certaines d‚Äôentre elles, comme le PIB, pr√©sentaient **une forte corr√©lation** avec d'autres indicateurs. Cette d√©pendance a compliqu√© la mise en place d‚Äôun mod√®le √©quilibr√©, capable de capturer des informations sans se concentrer excessivement sur une seule variable.
                Nous avons d√ª √©galement introduire d'autres variables pour enrichir les donn√©es.
                
                Certaines t√¢ches, en particulier **l‚Äôoptimisation des hyperparam√®tres** pour les mod√®les GBR et XGBoost, ont n√©cessit√© davantage de temps que pr√©vu. 
                Les tests d‚Äôoptimisation via **GridSearchCV** ont impliqu√© plusieurs it√©rations longues, et le mod√®le GBR a notamment pris plus d‚Äôune heure √† charger pour chaque ensemble d‚Äôhyperparam√®tres.
                                                 
                Les **scores finaux (R¬≤ proches de 0,89 et RMSE autour de 0,35)** d√©montrent une capacit√© de pr√©diction solide. Compar√©s aux benchmarks des mod√®les de base, tels que la r√©gression lin√©aire, GBR et XGBoost offrent une g√©n√©ralisation plus performante. Les objectifs ont ainsi √©t√© atteints avec **un mod√®le de pr√©diction fiable**, exploitable pour des √©tudes socio-√©conomiques sur la qualit√© de vie. 
                Ces r√©sultats peuvent √™tre int√©gr√©s dans des processus m√©tiers li√©s √† la pr√©vision de bien-√™tre, tels que des analyses pour des projets de d√©veloppement, des √©tudes de politique publique, ou des indices de qualit√© de vie, en utilisant les variables macro√©conomiques et sociales influentes identifi√©es.""")
        
        #afficher les scores des mod√®les optimis√©s
        if st.checkbox("Afficher les scores des deux mod√®les optimis√©s"):
            # Cr√©ation du DataFrame des r√©sultats
            data2 = {
            '': ['Score train', 'Score test  R¬≤', 'MSE','MAE','RMSE'],
            'GBR optimis√©': [0.9851,0.8876,0.1360,0.2839,0.3688],
            'XGB optimis√©': [0.9996, 0.8986, 0.1227,0.2602,0.3504]
                }
            metrics2 = pd.DataFrame(data2)
            st.dataframe(metrics2)

        st.write(" ### Pistes d'am√©lioration")
        st.write(" Pour am√©liorer la performance des mod√®les, plusieurs pistes peuvent √™tre envisag√©es :") 
        st.write("""
                - **Augmentation des donn√©es** : Int√©grer davantage de donn√©es ou de sources externes pour limiter la variance des pr√©dictions et enrichir les caract√©ristiques.      
                - **Assemblage de mod√®les** : Utiliser des techniques d'assemblage (stacking ou bagging) pour combiner les forces de GBR et XGBoost.        
                - **Optimisation avanc√©e des hyperparam√®tres** : Envisager des techniques de recherche bay√©sienne pour une exploration plus fine des param√®tres.     
                - **Ajout de variables explicatives** : Introduire des donn√©es sur les facteurs culturels, √©ducatifs, ou politiques pour enrichir les pr√©dictions.
            """)

    elif st.session_state.selected_page == "Auteurs":

        # Auteur 1: Agnesa Kurusina
        st.subheader("Agnesa Kurusina")
        st.write("""
        D'origine en Commerce international, je suis actuellement Responsable du d√©veloppement commercial dans un h√¥tel 4 √©toiles √† Biarritz. Mes missions principales consistent √† optimiser le chiffre d'affaires de l'entreprise gr√¢ce √† l'analyse de la performance et au benchmarking du march√©, ainsi qu'√† la gestion du portefeuille professionnel de l'√©tablissement, notamment pour l'accueil de groupes et de s√©minaires. Afin d'am√©liorer mes performances, celles de mon entreprise, et de d√©velopper mes opportunit√©s pour l'avenir, j'ai entrepris une formation de Data Analyst avec l'organisme Data Scientest. Mon objectif est d'apporter une approche ax√©e sur les donn√©es pour r√©soudre des probl√©matiques complexes et contribuer √† des projets ambitieux, en alliant mes comp√©tences techniques et m√©tier pour orienter le business vers un mod√®le Data Driven. En parall√®le, je m'investis dans des projets personnels li√©s √† la langue japonaise, la cuisine et le blogging, dans un esprit d'am√©lioration continue et de d√©veloppement personnel.""")
        st.markdown("[LinkedIn](https://www.linkedin.com/in/agnesa-kurusina-0a1804144)")

        # Auteur 2: Benjamin Giraud Matteoli
        st.subheader("Benjamin Giraud Matteoli")
        st.write("""
        Bonjour, je m'appelle Benjamin Giraud Matteoli. Fort de plus de 7 ans d'exp√©rience en finance, audit et conseil, 
    j'accompagne les directions financi√®res dans la mod√©lisation, le reporting et la migration de syst√®mes d‚Äôinformation. 
    Ayant travaill√© pour des groupes internationaux et ma√Ætrisant les enjeux de post-int√©gration et de conformit√©, 
    j'ai d√©cid√© de d√©velopper de nouvelles comp√©tences que j'estime √©l√©mentaires aujourd'hui en tant que Data Analyst 
    afin de compl√©ter mes comp√©tences m√©tiers avec des skills plus techniques.
    """)
        st.markdown("[LinkedIn](https://www.linkedin.com/in/benjamin-giraud-matteoli-57942775/)")

        # Auteur 3: Marie Pr√©v√¥t
        st.subheader("Marie Pr√©v√¥t")
        st.write("""
        Titulaire d‚Äôun dipl√¥me de master en Biologie, sp√©cialit√© Neurosciences cellulaires et int√©gratives, j‚Äôai pu √©toffer mon profil au travers d‚Äôexp√©riences professionnelles vari√©es, que ce soit dans le domaine de l‚Äô√©cologie o√π j‚Äôai pu mener √† bien diff√©rent projets en tant que charg√©e de mission ou dans l‚Äôoptique o√π mon travail consistait √† accompagner le client dans la d√©finition de son besoin et d‚Äôy r√©pondre avec la solution technique la plus adapt√©e.
    N‚Äô√©tant pas tout √† fait √©panouie dans ce secteur je souhaite aujourd‚Äôhui me tourner vers la data, qui me semble plus en phase avec mon go√ªt pour les chiffres et le sens de l‚Äôanalyse.""")

        # Auteur 4: Nimol Mann
        st.subheader("Nimol Mann")
        st.write("""
        Fort de 20 ans d‚Äôexp√©rience en supply chain approvisionnement et une app√©tence particuli√®re pour la data, j'ai r√©guli√®rement manipul√© des donn√©es (extractions datawarehouse, Access, Excel et BI) pour construire des analyses, tableaux de bord, KPIs et rapports.
    En reconversion dans le domaine de la data, je suis la formation Data Analyst chez Datascientest""")
        st.markdown("[LinkedIn](https://www.linkedin.com/in/nimol-mann-30a4a2112/)")

        

else:
    st.image("1690960666491.png")
    st.markdown(
        """
        <div style='text-align: center; font-size: 16px; line-height: 1.6;'>
            Ce projet vise √† <strong>pr√©dire le score de bonheur des pays</strong> √† partir de diverses caract√©ristiques socio-√©conomiques, 
            en s'appuyant sur des donn√©es historiques et des techniques de machine learning. <br><br> </div>""", unsafe_allow_html=True)

    st.markdown(
        """
        <div style='text-align: center; font-size: 16px; line-height: 1.6;'>
            En utilisant des indicateurs comme le <strong>PIB par habitant</strong>, <strong>le soutien social</strong>, <strong>l‚Äôesp√©rance de vie</strong>, 
            <strong>la perception de la libert√©</strong> et autres, le mod√®le tente d‚Äôestimer un <strong>score de bien-√™tre national</strong>. <br><br> </div>""", unsafe_allow_html=True)

    st.markdown(
        """
        <div style='text-align: center; font-size: 16px; line-height: 1.6;'>
            La m√©thodologie comprend <strong>le pr√©traitement de donn√©es</strong> pour assurer leur qualit√© et coh√©rence, <strong>la standardisation des variables</strong>, 
            et <strong>l'entra√Ænement de 5 mod√®les</strong>, puis ajustement des hyperparam√®tres pour trouver le meilleur algorithme de Machine Learning. <br><br>
            Le tableau de bord interactif offre <strong>une visualisation</strong> ainsi qu'une <strong>simulation dynamique de pr√©dictions</strong>, 
            vous permettant d‚Äôexplorer l'impact de chaque indicateur sur le bien-√™tre des pays.<br><br> </div>""", unsafe_allow_html=True)

    st.markdown(
        """
        <div style='text-align: center; font-size: 18px; line-height: 1.6; color: green;'>
        <strong>Utilisez le menu de gauche pour naviguer entre les sections.</strong>
        </div>""", unsafe_allow_html=True)

